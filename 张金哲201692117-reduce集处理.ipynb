{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4))\n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "base_dir = './dogImages'\n",
    "train_dir = os.path.join(base_dir, 'train_plus')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强数据集的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(train_plus_dir):\n",
    "#     shutil.copytree(train_dir,train_plus_dir)\n",
    "# for kind in os.listdir(train_plus_dir):\n",
    "#     kind_dir=os.path.join(train_plus_dir, kind)\n",
    "#     if len(os.listdir(kind_dir))*4 < 5*len([x for x in os.listdir(kind_dir) if x.startswith('pic_plus')])+5:\n",
    "#         continue\n",
    "#     for pic in os.listdir(kind_dir):\n",
    "#         pic_dir=os.path.join(kind_dir, pic)\n",
    "#         if not pic.endswith(\".jpg\"):\n",
    "#             continue\n",
    "#         if pic.startswith('pic_plus'):\n",
    "#             os.remove(pic_dir)\n",
    "#             continue\n",
    "#         print (pic_dir)\n",
    "#         image = load_img(pic_dir)\n",
    "#         image = img_to_array(image)\n",
    "#         image = np.expand_dims(image, axis=0)\n",
    "#         aug = ImageDataGenerator(rotation_range=30, \n",
    "#                                  width_shift_range=0.1,\n",
    "#                                  height_shift_range=0.1, \n",
    "#                                  shear_range=0.1, \n",
    "#                                  zoom_range=0.2,\n",
    "#                                  horizontal_flip=True, \n",
    "#                                  fill_mode=\"nearest\")\n",
    "#         total = 0\n",
    "#         imageGen = aug.flow(image, batch_size=1, \n",
    "#                             save_to_dir=kind_dir,\n",
    "#                             save_prefix=\"pic_plus\", \n",
    "#                             save_format=\"jpg\")\n",
    "#         print (imageGen)\n",
    "#         for image in imageGen:\n",
    "#         # increment our counter\n",
    "#             total += 1\n",
    "#             # if we have reached 4 examples, break from the loop\n",
    "#             if total == 4:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir=train_plus_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(256, 256, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 8, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               4358277   \n",
      "=================================================================\n",
      "Total params: 19,072,965\n",
      "Trainable params: 19,072,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(133, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# lr: float >= 0. 学习率 Learning rate\n",
    "\n",
    "# momentum: float >= 0. 参数更新动量 parameter updates momentum\n",
    "\n",
    "# decay: float >= 0. 学习率每次更新的下降率 Learning rate decay over each update\n",
    "\n",
    "# nesterov: boolean. 是否应用 Nesterov 动量 whether to apply Nesterov momentum\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20417 images belonging to 133 classes.\n",
      "Found 835 images belonging to 133 classes.\n",
      "{'001.Affenpinscher': 0, '002.Afghan_hound': 1, '003.Airedale_terrier': 2, '004.Akita': 3, '005.Alaskan_malamute': 4, '006.American_eskimo_dog': 5, '007.American_foxhound': 6, '008.American_staffordshire_terrier': 7, '009.American_water_spaniel': 8, '010.Anatolian_shepherd_dog': 9, '011.Australian_cattle_dog': 10, '012.Australian_shepherd': 11, '013.Australian_terrier': 12, '014.Basenji': 13, '015.Basset_hound': 14, '016.Beagle': 15, '017.Bearded_collie': 16, '018.Beauceron': 17, '019.Bedlington_terrier': 18, '020.Belgian_malinois': 19, '021.Belgian_sheepdog': 20, '022.Belgian_tervuren': 21, '023.Bernese_mountain_dog': 22, '024.Bichon_frise': 23, '025.Black_and_tan_coonhound': 24, '026.Black_russian_terrier': 25, '027.Bloodhound': 26, '028.Bluetick_coonhound': 27, '029.Border_collie': 28, '030.Border_terrier': 29, '031.Borzoi': 30, '032.Boston_terrier': 31, '033.Bouvier_des_flandres': 32, '034.Boxer': 33, '035.Boykin_spaniel': 34, '036.Briard': 35, '037.Brittany': 36, '038.Brussels_griffon': 37, '039.Bull_terrier': 38, '040.Bulldog': 39, '041.Bullmastiff': 40, '042.Cairn_terrier': 41, '043.Canaan_dog': 42, '044.Cane_corso': 43, '045.Cardigan_welsh_corgi': 44, '046.Cavalier_king_charles_spaniel': 45, '047.Chesapeake_bay_retriever': 46, '048.Chihuahua': 47, '049.Chinese_crested': 48, '050.Chinese_shar-pei': 49, '051.Chow_chow': 50, '052.Clumber_spaniel': 51, '053.Cocker_spaniel': 52, '054.Collie': 53, '055.Curly-coated_retriever': 54, '056.Dachshund': 55, '057.Dalmatian': 56, '058.Dandie_dinmont_terrier': 57, '059.Doberman_pinscher': 58, '060.Dogue_de_bordeaux': 59, '061.English_cocker_spaniel': 60, '062.English_setter': 61, '063.English_springer_spaniel': 62, '064.English_toy_spaniel': 63, '065.Entlebucher_mountain_dog': 64, '066.Field_spaniel': 65, '067.Finnish_spitz': 66, '068.Flat-coated_retriever': 67, '069.French_bulldog': 68, '070.German_pinscher': 69, '071.German_shepherd_dog': 70, '072.German_shorthaired_pointer': 71, '073.German_wirehaired_pointer': 72, '074.Giant_schnauzer': 73, '075.Glen_of_imaal_terrier': 74, '076.Golden_retriever': 75, '077.Gordon_setter': 76, '078.Great_dane': 77, '079.Great_pyrenees': 78, '080.Greater_swiss_mountain_dog': 79, '081.Greyhound': 80, '082.Havanese': 81, '083.Ibizan_hound': 82, '084.Icelandic_sheepdog': 83, '085.Irish_red_and_white_setter': 84, '086.Irish_setter': 85, '087.Irish_terrier': 86, '088.Irish_water_spaniel': 87, '089.Irish_wolfhound': 88, '090.Italian_greyhound': 89, '091.Japanese_chin': 90, '092.Keeshond': 91, '093.Kerry_blue_terrier': 92, '094.Komondor': 93, '095.Kuvasz': 94, '096.Labrador_retriever': 95, '097.Lakeland_terrier': 96, '098.Leonberger': 97, '099.Lhasa_apso': 98, '100.Lowchen': 99, '101.Maltese': 100, '102.Manchester_terrier': 101, '103.Mastiff': 102, '104.Miniature_schnauzer': 103, '105.Neapolitan_mastiff': 104, '106.Newfoundland': 105, '107.Norfolk_terrier': 106, '108.Norwegian_buhund': 107, '109.Norwegian_elkhound': 108, '110.Norwegian_lundehund': 109, '111.Norwich_terrier': 110, '112.Nova_scotia_duck_tolling_retriever': 111, '113.Old_english_sheepdog': 112, '114.Otterhound': 113, '115.Papillon': 114, '116.Parson_russell_terrier': 115, '117.Pekingese': 116, '118.Pembroke_welsh_corgi': 117, '119.Petit_basset_griffon_vendeen': 118, '120.Pharaoh_hound': 119, '121.Plott': 120, '122.Pointer': 121, '123.Pomeranian': 122, '124.Poodle': 123, '125.Portuguese_water_dog': 124, '126.Saint_bernard': 125, '127.Silky_terrier': 126, '128.Smooth_fox_terrier': 127, '129.Tibetan_mastiff': 128, '130.Welsh_springer_spaniel': 129, '131.Wirehaired_pointing_griffon': 130, '132.Xoloitzcuintli': 131, '133.Yorkshire_terrier': 132}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(256, 256),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=100,\n",
    "        class_mode='categorical')\n",
    "validation_generator\n",
    "print(validation_generator.class_indices)\n",
    "np.save('labels.npy', validation_generator.class_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 256, 256, 3)\n",
      "labels batch shape: (20, 133)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 59s 585ms/step - loss: 3.0403 - acc: 0.2840 - val_loss: 3.2838 - val_acc: 0.2278\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.28377, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 2.8890 - acc: 0.3035 - val_loss: 3.0993 - val_acc: 0.2629\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.28377 to 3.09933, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 2.6620 - acc: 0.3635 - val_loss: 3.0227 - val_acc: 0.2740\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.09933 to 3.02275, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 2.6952 - acc: 0.3580 - val_loss: 2.9147 - val_acc: 0.2978\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.02275 to 2.91470, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 2.5231 - acc: 0.3830 - val_loss: 2.9207 - val_acc: 0.2889\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.91470\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 59s 591ms/step - loss: 1.5140 - acc: 0.6630 - val_loss: 2.8264 - val_acc: 0.2967\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.91470 to 2.82642, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 57s 567ms/step - loss: 1.4583 - acc: 0.6775 - val_loss: 2.8618 - val_acc: 0.2987\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.82642\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 1.4379 - acc: 0.6675 - val_loss: 2.8283 - val_acc: 0.3174\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.82642\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 59s 585ms/step - loss: 1.3948 - acc: 0.6745 - val_loss: 2.7411 - val_acc: 0.3252\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.82642 to 2.74115, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 1.3740 - acc: 0.6645 - val_loss: 2.7743 - val_acc: 0.3315\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.74115\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 1.4029 - acc: 0.6540 - val_loss: 2.7419 - val_acc: 0.3328\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.74115\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 57s 565ms/step - loss: 1.3437 - acc: 0.6620 - val_loss: 2.7800 - val_acc: 0.3362\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.74115\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 58s 582ms/step - loss: 1.2884 - acc: 0.6885 - val_loss: 2.7403 - val_acc: 0.3559\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.74115 to 2.74029, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 57s 568ms/step - loss: 1.2641 - acc: 0.6820 - val_loss: 2.6817 - val_acc: 0.3495\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.74029 to 2.68166, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 58s 584ms/step - loss: 1.3208 - acc: 0.6595 - val_loss: 2.6665 - val_acc: 0.3472\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.68166 to 2.66651, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 0.8938 - acc: 0.8075 - val_loss: 2.7509 - val_acc: 0.3614\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.66651\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 58s 576ms/step - loss: 0.7262 - acc: 0.8365 - val_loss: 2.6754 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.66651\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 58s 584ms/step - loss: 0.7294 - acc: 0.8290 - val_loss: 2.7508 - val_acc: 0.3395\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.66651\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.6985 - acc: 0.8410 - val_loss: 2.6447 - val_acc: 0.3629\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.66651 to 2.64466, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.7459 - acc: 0.8245 - val_loss: 2.6540 - val_acc: 0.3476\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.64466\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.7440 - acc: 0.8200 - val_loss: 2.6677 - val_acc: 0.3692\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.64466\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 59s 595ms/step - loss: 0.7114 - acc: 0.8300 - val_loss: 2.6351 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.64466 to 2.63513, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 56s 564ms/step - loss: 0.7097 - acc: 0.8235 - val_loss: 2.6657 - val_acc: 0.3495\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.63513\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 0.7328 - acc: 0.8235 - val_loss: 2.6284 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.63513 to 2.62839, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 0.7438 - acc: 0.8020 - val_loss: 2.6209 - val_acc: 0.3824\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.62839 to 2.62086, saving model to dogskind_reduce_expand.model.weights.best.hdf5\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 0.5177 - acc: 0.8805 - val_loss: 2.6246 - val_acc: 0.3786\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.62086\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.4074 - acc: 0.9145 - val_loss: 2.6486 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.62086\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.3940 - acc: 0.9200 - val_loss: 2.6407 - val_acc: 0.3777\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.62086\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 59s 591ms/step - loss: 0.4121 - acc: 0.9085 - val_loss: 2.7128 - val_acc: 0.3863\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.62086\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 56s 559ms/step - loss: 0.4371 - acc: 0.8960 - val_loss: 2.6269 - val_acc: 0.3918\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.62086\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 59s 589ms/step - loss: 0.4079 - acc: 0.9060 - val_loss: 2.6378 - val_acc: 0.3904\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.62086\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 0.3977 - acc: 0.9105 - val_loss: 2.6412 - val_acc: 0.3978\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.62086\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from PIL import ImageFile\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=7) \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "checkpointer = ModelCheckpoint(filepath='dogskind_reduce_expand.model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "history = model.fit_generator(train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50, \n",
    "      callbacks=[early_stopping,checkpointer], \n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "      steps_per_epoch=20,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50, \n",
    "      callbacks=[early_stopping,checkpointer], \n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('dogskind_reduce_expand.model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 836 images belonging to 133 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3293092399835587, 0.421875]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=40,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.evaluate_generator(test_generator,steps=8, max_queue_size=50, workers=1, use_multiprocessing=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_string = model.to_yaml()\n",
    "with open('./0cp.yaml', 'w') as outfile:\n",
    "    outfile.write(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
