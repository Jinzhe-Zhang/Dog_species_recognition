{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4))\n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "base_dir = './dogImages'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "train_plus_dir=os.path.join(base_dir, 'train_plus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强数据集的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(train_plus_dir):\n",
    "#     shutil.copytree(train_dir,train_plus_dir)\n",
    "# for kind in os.listdir(train_plus_dir):\n",
    "#     kind_dir=os.path.join(train_plus_dir, kind)\n",
    "#     if len(os.listdir(kind_dir))*4 < 5*len([x for x in os.listdir(kind_dir) if x.startswith('pic_plus')])+5:\n",
    "#         continue\n",
    "#     for pic in os.listdir(kind_dir):\n",
    "#         pic_dir=os.path.join(kind_dir, pic)\n",
    "#         if not pic.endswith(\".jpg\"):\n",
    "#             continue\n",
    "#         if pic.startswith('pic_plus'):\n",
    "#             os.remove(pic_dir)\n",
    "#             continue\n",
    "#         print (pic_dir)\n",
    "#         image = load_img(pic_dir)\n",
    "#         image = img_to_array(image)\n",
    "#         image = np.expand_dims(image, axis=0)\n",
    "#         aug = ImageDataGenerator(rotation_range=30, \n",
    "#                                  width_shift_range=0.1,\n",
    "#                                  height_shift_range=0.1, \n",
    "#                                  shear_range=0.1, \n",
    "#                                  zoom_range=0.2,\n",
    "#                                  horizontal_flip=True, \n",
    "#                                  fill_mode=\"nearest\")\n",
    "#         total = 0\n",
    "#         imageGen = aug.flow(image, batch_size=1, \n",
    "#                             save_to_dir=kind_dir,\n",
    "#                             save_prefix=\"pic_plus\", \n",
    "#                             save_format=\"jpg\")\n",
    "#         print (imageGen)\n",
    "#         for image in imageGen:\n",
    "#         # increment our counter\n",
    "#             total += 1\n",
    "#             # if we have reached 4 examples, break from the loop\n",
    "#             if total == 4:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=train_plus_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoming/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 64, 64, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 64, 64, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 64, 64, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 32, 32, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 32, 32, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 32, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 16, 16, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 8, 8, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 2048)   0           add_16[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import resnet50\n",
    "\n",
    "conv_base = resnet50.ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(256, 256, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 8, 8, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               17432709  \n",
      "=================================================================\n",
      "Total params: 41,020,421\n",
      "Trainable params: 40,967,301\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(133, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# lr: float >= 0. 学习率 Learning rate\n",
    "\n",
    "# momentum: float >= 0. 参数更新动量 parameter updates momentum\n",
    "\n",
    "# decay: float >= 0. 学习率每次更新的下降率 Learning rate decay over each update\n",
    "\n",
    "# nesterov: boolean. 是否应用 Nesterov 动量 whether to apply Nesterov momentum\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-6),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20417 images belonging to 133 classes.\n",
      "Found 835 images belonging to 133 classes.\n",
      "{'001.Affenpinscher': 0, '002.Afghan_hound': 1, '003.Airedale_terrier': 2, '004.Akita': 3, '005.Alaskan_malamute': 4, '006.American_eskimo_dog': 5, '007.American_foxhound': 6, '008.American_staffordshire_terrier': 7, '009.American_water_spaniel': 8, '010.Anatolian_shepherd_dog': 9, '011.Australian_cattle_dog': 10, '012.Australian_shepherd': 11, '013.Australian_terrier': 12, '014.Basenji': 13, '015.Basset_hound': 14, '016.Beagle': 15, '017.Bearded_collie': 16, '018.Beauceron': 17, '019.Bedlington_terrier': 18, '020.Belgian_malinois': 19, '021.Belgian_sheepdog': 20, '022.Belgian_tervuren': 21, '023.Bernese_mountain_dog': 22, '024.Bichon_frise': 23, '025.Black_and_tan_coonhound': 24, '026.Black_russian_terrier': 25, '027.Bloodhound': 26, '028.Bluetick_coonhound': 27, '029.Border_collie': 28, '030.Border_terrier': 29, '031.Borzoi': 30, '032.Boston_terrier': 31, '033.Bouvier_des_flandres': 32, '034.Boxer': 33, '035.Boykin_spaniel': 34, '036.Briard': 35, '037.Brittany': 36, '038.Brussels_griffon': 37, '039.Bull_terrier': 38, '040.Bulldog': 39, '041.Bullmastiff': 40, '042.Cairn_terrier': 41, '043.Canaan_dog': 42, '044.Cane_corso': 43, '045.Cardigan_welsh_corgi': 44, '046.Cavalier_king_charles_spaniel': 45, '047.Chesapeake_bay_retriever': 46, '048.Chihuahua': 47, '049.Chinese_crested': 48, '050.Chinese_shar-pei': 49, '051.Chow_chow': 50, '052.Clumber_spaniel': 51, '053.Cocker_spaniel': 52, '054.Collie': 53, '055.Curly-coated_retriever': 54, '056.Dachshund': 55, '057.Dalmatian': 56, '058.Dandie_dinmont_terrier': 57, '059.Doberman_pinscher': 58, '060.Dogue_de_bordeaux': 59, '061.English_cocker_spaniel': 60, '062.English_setter': 61, '063.English_springer_spaniel': 62, '064.English_toy_spaniel': 63, '065.Entlebucher_mountain_dog': 64, '066.Field_spaniel': 65, '067.Finnish_spitz': 66, '068.Flat-coated_retriever': 67, '069.French_bulldog': 68, '070.German_pinscher': 69, '071.German_shepherd_dog': 70, '072.German_shorthaired_pointer': 71, '073.German_wirehaired_pointer': 72, '074.Giant_schnauzer': 73, '075.Glen_of_imaal_terrier': 74, '076.Golden_retriever': 75, '077.Gordon_setter': 76, '078.Great_dane': 77, '079.Great_pyrenees': 78, '080.Greater_swiss_mountain_dog': 79, '081.Greyhound': 80, '082.Havanese': 81, '083.Ibizan_hound': 82, '084.Icelandic_sheepdog': 83, '085.Irish_red_and_white_setter': 84, '086.Irish_setter': 85, '087.Irish_terrier': 86, '088.Irish_water_spaniel': 87, '089.Irish_wolfhound': 88, '090.Italian_greyhound': 89, '091.Japanese_chin': 90, '092.Keeshond': 91, '093.Kerry_blue_terrier': 92, '094.Komondor': 93, '095.Kuvasz': 94, '096.Labrador_retriever': 95, '097.Lakeland_terrier': 96, '098.Leonberger': 97, '099.Lhasa_apso': 98, '100.Lowchen': 99, '101.Maltese': 100, '102.Manchester_terrier': 101, '103.Mastiff': 102, '104.Miniature_schnauzer': 103, '105.Neapolitan_mastiff': 104, '106.Newfoundland': 105, '107.Norfolk_terrier': 106, '108.Norwegian_buhund': 107, '109.Norwegian_elkhound': 108, '110.Norwegian_lundehund': 109, '111.Norwich_terrier': 110, '112.Nova_scotia_duck_tolling_retriever': 111, '113.Old_english_sheepdog': 112, '114.Otterhound': 113, '115.Papillon': 114, '116.Parson_russell_terrier': 115, '117.Pekingese': 116, '118.Pembroke_welsh_corgi': 117, '119.Petit_basset_griffon_vendeen': 118, '120.Pharaoh_hound': 119, '121.Plott': 120, '122.Pointer': 121, '123.Pomeranian': 122, '124.Poodle': 123, '125.Portuguese_water_dog': 124, '126.Saint_bernard': 125, '127.Silky_terrier': 126, '128.Smooth_fox_terrier': 127, '129.Tibetan_mastiff': 128, '130.Welsh_springer_spaniel': 129, '131.Wirehaired_pointing_griffon': 130, '132.Xoloitzcuintli': 131, '133.Yorkshire_terrier': 132}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(256, 256),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=100,\n",
    "        class_mode='categorical')\n",
    "validation_generator\n",
    "print(validation_generator.class_indices)\n",
    "np.save('labels.npy', validation_generator.class_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 256, 256, 3)\n",
      "labels batch shape: (20, 133)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 142s 354ms/step - loss: 6.4893 - acc: 0.0151 - val_loss: 5.8483 - val_acc: 0.0229\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.84834, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 131s 329ms/step - loss: 5.4670 - acc: 0.0479 - val_loss: 4.9366 - val_acc: 0.0694\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.84834 to 4.93662, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 132s 330ms/step - loss: 4.3984 - acc: 0.1310 - val_loss: 4.2053 - val_acc: 0.1425\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.93662 to 4.20526, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 132s 329ms/step - loss: 3.5543 - acc: 0.2176 - val_loss: 3.5782 - val_acc: 0.2195\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.20526 to 3.57820, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 123s 308ms/step - loss: 3.0298 - acc: 0.2878 - val_loss: 3.1240 - val_acc: 0.2879\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.57820 to 3.12398, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 2.3648 - acc: 0.4033 - val_loss: 2.7410 - val_acc: 0.3432\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.12398 to 2.74103, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 2.0107 - acc: 0.4756 - val_loss: 2.4928 - val_acc: 0.3799\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.74103 to 2.49284, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 1.7118 - acc: 0.5380 - val_loss: 2.2403 - val_acc: 0.4390\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.49284 to 2.24029, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 1.3781 - acc: 0.6128 - val_loss: 2.0893 - val_acc: 0.4675\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.24029 to 2.08929, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 1.2613 - acc: 0.6389 - val_loss: 1.9419 - val_acc: 0.4995\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.08929 to 1.94190, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 1.0020 - acc: 0.7077 - val_loss: 1.8177 - val_acc: 0.5069\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.94190 to 1.81770, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.8994 - acc: 0.7321 - val_loss: 1.7259 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.81770 to 1.72588, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 120s 301ms/step - loss: 0.8080 - acc: 0.7664 - val_loss: 1.6692 - val_acc: 0.5453\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.72588 to 1.66916, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.6489 - acc: 0.8103 - val_loss: 1.5823 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.66916 to 1.58231, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 0.6283 - acc: 0.8150 - val_loss: 1.5478 - val_acc: 0.5705\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.58231 to 1.54784, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.5123 - acc: 0.8496 - val_loss: 1.4762 - val_acc: 0.5771\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.54784 to 1.47616, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.4445 - acc: 0.8731 - val_loss: 1.4480 - val_acc: 0.5866\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.47616 to 1.44803, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.4151 - acc: 0.8787 - val_loss: 1.4058 - val_acc: 0.6004\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.44803 to 1.40585, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.3293 - acc: 0.9087 - val_loss: 1.3650 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.40585 to 1.36503, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 121s 301ms/step - loss: 0.2967 - acc: 0.9136 - val_loss: 1.3416 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.36503 to 1.34157, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.2529 - acc: 0.9327 - val_loss: 1.3087 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.34157 to 1.30865, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 120s 300ms/step - loss: 0.2302 - acc: 0.9380 - val_loss: 1.2961 - val_acc: 0.6243\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.30865 to 1.29611, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.2083 - acc: 0.9455 - val_loss: 1.2622 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.29611 to 1.26215, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 120s 299ms/step - loss: 0.1541 - acc: 0.9647 - val_loss: 1.2738 - val_acc: 0.6362\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.26215\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.1560 - acc: 0.9635 - val_loss: 1.2181 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.26215 to 1.21807, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 120s 300ms/step - loss: 0.1278 - acc: 0.9706 - val_loss: 1.2250 - val_acc: 0.6512\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.21807\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 119s 297ms/step - loss: 0.1049 - acc: 0.9785 - val_loss: 1.2000 - val_acc: 0.6534\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.21807 to 1.19996, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.1032 - acc: 0.9786 - val_loss: 1.1914 - val_acc: 0.6539\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.19996 to 1.19142, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.0763 - acc: 0.9860 - val_loss: 1.1841 - val_acc: 0.6625\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.19142 to 1.18412, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0713 - acc: 0.9869 - val_loss: 1.1617 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.18412 to 1.16171, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.0671 - acc: 0.9872 - val_loss: 1.1419 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.16171 to 1.14190, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 121s 304ms/step - loss: 0.0525 - acc: 0.9915 - val_loss: 1.1573 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.14190\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0490 - acc: 0.9934 - val_loss: 1.1264 - val_acc: 0.6703\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.14190 to 1.12640, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.0370 - acc: 0.9949 - val_loss: 1.1370 - val_acc: 0.6747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss did not improve from 1.12640\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0353 - acc: 0.9950 - val_loss: 1.1167 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.12640 to 1.11669, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0303 - acc: 0.9969 - val_loss: 1.1274 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.11669\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0257 - acc: 0.9977 - val_loss: 1.1235 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.11669\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0215 - acc: 0.9987 - val_loss: 1.0960 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.11669 to 1.09596, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0182 - acc: 0.9987 - val_loss: 1.0926 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.09596 to 1.09263, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 123s 308ms/step - loss: 0.0179 - acc: 0.9991 - val_loss: 1.1134 - val_acc: 0.6870\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.09263\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 118s 296ms/step - loss: 0.0165 - acc: 0.9984 - val_loss: 1.0909 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.09263 to 1.09095, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0123 - acc: 0.9992 - val_loss: 1.0835 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.09095 to 1.08355, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0117 - acc: 0.9990 - val_loss: 1.0617 - val_acc: 0.6997\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.08355 to 1.06171, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 121s 304ms/step - loss: 0.0096 - acc: 0.9995 - val_loss: 1.0679 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.06171\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0093 - acc: 0.9991 - val_loss: 1.0761 - val_acc: 0.7048\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.06171\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0085 - acc: 0.9991 - val_loss: 1.0581 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.06171 to 1.05809, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0070 - acc: 0.9994 - val_loss: 1.0694 - val_acc: 0.7015\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.05809\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0063 - acc: 0.9995 - val_loss: 1.0753 - val_acc: 0.7016\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.05809\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0052 - acc: 0.9997 - val_loss: 1.0497 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.05809 to 1.04973, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0048 - acc: 0.9997 - val_loss: 1.0539 - val_acc: 0.7099\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.04973\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 0.0047 - acc: 0.9994 - val_loss: 1.0423 - val_acc: 0.7115\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.04973 to 1.04232, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 120s 301ms/step - loss: 0.0038 - acc: 0.9995 - val_loss: 1.0603 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.04232\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 120s 300ms/step - loss: 0.0038 - acc: 0.9999 - val_loss: 1.0532 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.04232\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 119s 299ms/step - loss: 0.0028 - acc: 0.9997 - val_loss: 1.0482 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.04232\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0025 - acc: 0.9999 - val_loss: 1.0477 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.04232\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.0030 - acc: 0.9995 - val_loss: 1.0408 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.04232 to 1.04083, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.0340 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.04083 to 1.03403, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 125s 314ms/step - loss: 0.0021 - acc: 0.9997 - val_loss: 1.0438 - val_acc: 0.7187\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.03403\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 120s 300ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 1.0336 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.03403 to 1.03364, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.0505 - val_acc: 0.7180\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.03364\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 121s 302ms/step - loss: 0.0023 - acc: 0.9997 - val_loss: 1.0441 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.03364\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.0332 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.03364 to 1.03322, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 121s 301ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 1.0360 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.03322\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 124s 309ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 1.0330 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.03322 to 1.03303, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 121s 301ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 1.0362 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.03303\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 1.0402 - val_acc: 0.7348\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.03303\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 1.0200 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.03303 to 1.02001, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 122s 306ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 1.0333 - val_acc: 0.7320\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.02001\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 1.0326 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.02001\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 123s 307ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 1.0464 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.02001\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 1.0334 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.02001\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 1.0201 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.02001\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 122s 305ms/step - loss: 9.4509e-04 - acc: 0.9997 - val_loss: 1.0412 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.02001\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 122s 304ms/step - loss: 0.0011 - acc: 0.9994 - val_loss: 1.0291 - val_acc: 0.7373\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.02001\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from PIL import ImageFile\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=7) \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "checkpointer = ModelCheckpoint(filepath='dogskindresnet.model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "history = model.fit_generator(train_generator,\n",
    "      steps_per_epoch=400,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50, \n",
    "      callbacks=[early_stopping,checkpointer], \n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "100/100 [==============================] - 61s 615ms/step - loss: 6.6231e-04 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04116, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 2/10000\n",
      "100/100 [==============================] - 61s 611ms/step - loss: 5.4980e-04 - acc: 1.0000 - val_loss: 1.0325 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04116 to 1.03248, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 3/10000\n",
      "100/100 [==============================] - 65s 646ms/step - loss: 9.3114e-04 - acc: 0.9995 - val_loss: 1.0369 - val_acc: 0.7363\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.03248\n",
      "Epoch 4/10000\n",
      "100/100 [==============================] - 64s 638ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 1.0167 - val_acc: 0.7343\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.03248 to 1.01674, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 5/10000\n",
      "100/100 [==============================] - 63s 633ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 1.0293 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01674\n",
      "Epoch 6/10000\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 1.0268 - val_acc: 0.7335\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.01674\n",
      "Epoch 7/10000\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 1.0286 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.01674\n",
      "Epoch 8/10000\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 1.0351 - val_acc: 0.7337\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.01674\n",
      "Epoch 9/10000\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 1.0304 - val_acc: 0.7343\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.01674\n",
      "Epoch 10/10000\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 7.9990e-04 - acc: 1.0000 - val_loss: 1.0216 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.01674\n",
      "Epoch 11/10000\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 8.6400e-04 - acc: 1.0000 - val_loss: 1.0300 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.01674\n",
      "Epoch 12/10000\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 1.0403 - val_acc: 0.7318\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.01674\n",
      "Epoch 13/10000\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.0364 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.01674\n",
      "Epoch 14/10000\n",
      "100/100 [==============================] - 59s 595ms/step - loss: 5.7353e-04 - acc: 1.0000 - val_loss: 1.0374 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.01674\n",
      "Epoch 15/10000\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 3.8424e-04 - acc: 1.0000 - val_loss: 1.0227 - val_acc: 0.7446\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.01674\n",
      "Epoch 16/10000\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 5.3259e-04 - acc: 1.0000 - val_loss: 1.0435 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.01674\n",
      "Epoch 17/10000\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 3.4164e-04 - acc: 1.0000 - val_loss: 1.0396 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.01674\n",
      "Epoch 18/10000\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 7.2334e-04 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.01674\n",
      "Epoch 19/10000\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 1.0404 - val_acc: 0.7403\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.01674\n",
      "Epoch 20/10000\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 8.1641e-04 - acc: 0.9995 - val_loss: 1.0128 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.01674 to 1.01281, saving model to dogskindresnet.model.weights.best.hdf5\n",
      "Epoch 21/10000\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 8.2937e-04 - acc: 0.9995 - val_loss: 1.0427 - val_acc: 0.7352\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.01281\n",
      "Epoch 22/10000\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.0023 - acc: 0.9990 - val_loss: 1.0323 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.01281\n",
      "Epoch 23/10000\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 3.2099e-04 - acc: 1.0000 - val_loss: 1.0168 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.01281\n",
      "Epoch 24/10000\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 1.0268 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.01281\n",
      "Epoch 25/10000\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 3.9310e-04 - acc: 1.0000 - val_loss: 1.0374 - val_acc: 0.7393\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.01281\n",
      "Epoch 26/10000\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 1.0376 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.01281\n",
      "Epoch 27/10000\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.0223 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.01281\n",
      "Epoch 28/10000\n",
      "100/100 [==============================] - 60s 595ms/step - loss: 4.1228e-04 - acc: 1.0000 - val_loss: 1.0385 - val_acc: 0.7393\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.01281\n",
      "Epoch 29/10000\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 3.0566e-04 - acc: 1.0000 - val_loss: 1.0236 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.01281\n",
      "Epoch 30/10000\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 2.7595e-04 - acc: 1.0000 - val_loss: 1.0456 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.01281\n",
      "Epoch 31/10000\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 1.0367 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.01281\n",
      "Epoch 32/10000\n",
      "100/100 [==============================] - 61s 613ms/step - loss: 3.3485e-04 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.01281\n",
      "Epoch 33/10000\n",
      "100/100 [==============================] - 61s 614ms/step - loss: 6.1598e-04 - acc: 1.0000 - val_loss: 1.0387 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.01281\n",
      "Epoch 34/10000\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 4.4849e-04 - acc: 1.0000 - val_loss: 1.0372 - val_acc: 0.7373\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.01281\n",
      "Epoch 35/10000\n",
      "100/100 [==============================] - 62s 625ms/step - loss: 2.9831e-04 - acc: 1.0000 - val_loss: 1.0342 - val_acc: 0.7403\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.01281\n",
      "Epoch 36/10000\n",
      "100/100 [==============================] - 62s 615ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 1.0411 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.01281\n",
      "Epoch 37/10000\n",
      "100/100 [==============================] - 62s 624ms/step - loss: 0.0028 - acc: 0.9995 - val_loss: 1.0354 - val_acc: 0.7399\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.01281\n",
      "Epoch 38/10000\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 1.0401 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.01281\n",
      "Epoch 39/10000\n",
      "100/100 [==============================] - 64s 635ms/step - loss: 2.0456e-04 - acc: 1.0000 - val_loss: 1.0353 - val_acc: 0.7373\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.01281\n",
      "Epoch 40/10000\n",
      "100/100 [==============================] - 62s 624ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 1.0396 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.01281\n",
      "Epoch 41/10000\n",
      "100/100 [==============================] - 61s 606ms/step - loss: 1.9365e-04 - acc: 1.0000 - val_loss: 1.0319 - val_acc: 0.7403\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.01281\n",
      "Epoch 42/10000\n",
      "100/100 [==============================] - 61s 610ms/step - loss: 2.2626e-04 - acc: 1.0000 - val_loss: 1.0454 - val_acc: 0.7354\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.01281\n",
      "Epoch 43/10000\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 1.8857e-04 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.01281\n",
      "Epoch 44/10000\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 2.1045e-04 - acc: 1.0000 - val_loss: 1.0350 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.01281\n",
      "Epoch 45/10000\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 8.2094e-04 - acc: 0.9995 - val_loss: 1.0312 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.01281\n",
      "Epoch 46/10000\n",
      "100/100 [==============================] - 62s 625ms/step - loss: 2.0650e-04 - acc: 1.0000 - val_loss: 1.0231 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.01281\n",
      "Epoch 47/10000\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 9.3536e-04 - acc: 0.9995 - val_loss: 1.0419 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.01281\n",
      "Epoch 48/10000\n",
      "100/100 [==============================] - 62s 621ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 1.0398 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.01281\n",
      "Epoch 49/10000\n",
      "100/100 [==============================] - 62s 618ms/step - loss: 0.0025 - acc: 0.9995 - val_loss: 1.0346 - val_acc: 0.7419\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.01281\n",
      "Epoch 50/10000\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 1.0388 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.01281\n",
      "Epoch 51/10000\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 1.0264 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.01281\n",
      "Epoch 52/10000\n",
      "100/100 [==============================] - 62s 623ms/step - loss: 1.2724e-04 - acc: 1.0000 - val_loss: 1.0482 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.01281\n",
      "Epoch 53/10000\n",
      "100/100 [==============================] - 63s 630ms/step - loss: 0.0015 - acc: 0.9990 - val_loss: 1.0444 - val_acc: 0.7412\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.01281\n",
      "Epoch 54/10000\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 1.4778e-04 - acc: 1.0000 - val_loss: 1.0311 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.01281\n",
      "Epoch 55/10000\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 6.0311e-04 - acc: 0.9995 - val_loss: 1.0625 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.01281\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=35) \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "checkpointer = ModelCheckpoint(filepath='dogskindresnet.model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "history = model.fit_generator(train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=10000,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50, \n",
    "      callbacks=[early_stopping,checkpointer], \n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('dogskindresnet.model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 836 images belonging to 133 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.007829131427993, 0.7807692305654542]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=100,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.evaluate_generator(test_generator,steps=50, max_queue_size=50, workers=1, use_multiprocessing=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import *\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_string = model.to_yaml()\n",
    "with open('./0.yaml', 'w') as outfile:\n",
    "    outfile.write(yaml_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
