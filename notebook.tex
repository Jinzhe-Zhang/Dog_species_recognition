
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{???201692117-Resnet??}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{os}\PY{o}{,} \PY{n+nn}{shutil}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        \PY{k+kn}{import} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{backend}\PY{n+nn}{.}\PY{n+nn}{tensorflow\PYZus{}backend} \PY{k}{as} \PY{n+nn}{KTF}
        \PY{n}{config} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{ConfigProto}\PY{p}{(}\PY{n}{gpu\PYZus{}options}\PY{o}{=}\PY{n}{tf}\PY{o}{.}\PY{n}{GPUOptions}\PY{p}{(}\PY{n}{per\PYZus{}process\PYZus{}gpu\PYZus{}memory\PYZus{}fraction}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{)}
        \PY{n}{sess} \PY{o}{=} \PY{n}{tf}\PY{o}{.}\PY{n}{Session}\PY{p}{(}\PY{n}{config}\PY{o}{=}\PY{n}{config}\PY{p}{)}
        \PY{n}{KTF}\PY{o}{.}\PY{n}{set\PYZus{}session}\PY{p}{(}\PY{n}{sess}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{ImageDataGenerator}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{img\PYZus{}to\PYZus{}array}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{load\PYZus{}img}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{n}{base\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./dogImages}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{train\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{base\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{validation\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{base\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{base\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{train\PYZus{}plus\PYZus{}dir}\PY{o}{=}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{base\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}plus}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \section{增强数据集的操作}\label{ux589eux5f3aux6570ux636eux96c6ux7684ux64cdux4f5c}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} if not os.path.exists(train\PYZus{}plus\PYZus{}dir):}
        \PY{c+c1}{\PYZsh{}     shutil.copytree(train\PYZus{}dir,train\PYZus{}plus\PYZus{}dir)}
        \PY{c+c1}{\PYZsh{} for kind in os.listdir(train\PYZus{}plus\PYZus{}dir):}
        \PY{c+c1}{\PYZsh{}     kind\PYZus{}dir=os.path.join(train\PYZus{}plus\PYZus{}dir, kind)}
        \PY{c+c1}{\PYZsh{}     if len(os.listdir(kind\PYZus{}dir))*4 \PYZlt{} 5*len([x for x in os.listdir(kind\PYZus{}dir) if x.startswith(\PYZsq{}pic\PYZus{}plus\PYZsq{})])+5:}
        \PY{c+c1}{\PYZsh{}         continue}
        \PY{c+c1}{\PYZsh{}     for pic in os.listdir(kind\PYZus{}dir):}
        \PY{c+c1}{\PYZsh{}         pic\PYZus{}dir=os.path.join(kind\PYZus{}dir, pic)}
        \PY{c+c1}{\PYZsh{}         if not pic.endswith(\PYZdq{}.jpg\PYZdq{}):}
        \PY{c+c1}{\PYZsh{}             continue}
        \PY{c+c1}{\PYZsh{}         if pic.startswith(\PYZsq{}pic\PYZus{}plus\PYZsq{}):}
        \PY{c+c1}{\PYZsh{}             os.remove(pic\PYZus{}dir)}
        \PY{c+c1}{\PYZsh{}             continue}
        \PY{c+c1}{\PYZsh{}         print (pic\PYZus{}dir)}
        \PY{c+c1}{\PYZsh{}         image = load\PYZus{}img(pic\PYZus{}dir)}
        \PY{c+c1}{\PYZsh{}         image = img\PYZus{}to\PYZus{}array(image)}
        \PY{c+c1}{\PYZsh{}         image = np.expand\PYZus{}dims(image, axis=0)}
        \PY{c+c1}{\PYZsh{}         aug = ImageDataGenerator(rotation\PYZus{}range=30, }
        \PY{c+c1}{\PYZsh{}                                  width\PYZus{}shift\PYZus{}range=0.1,}
        \PY{c+c1}{\PYZsh{}                                  height\PYZus{}shift\PYZus{}range=0.1, }
        \PY{c+c1}{\PYZsh{}                                  shear\PYZus{}range=0.1, }
        \PY{c+c1}{\PYZsh{}                                  zoom\PYZus{}range=0.2,}
        \PY{c+c1}{\PYZsh{}                                  horizontal\PYZus{}flip=True, }
        \PY{c+c1}{\PYZsh{}                                  fill\PYZus{}mode=\PYZdq{}nearest\PYZdq{})}
        \PY{c+c1}{\PYZsh{}         total = 0}
        \PY{c+c1}{\PYZsh{}         imageGen = aug.flow(image, batch\PYZus{}size=1, }
        \PY{c+c1}{\PYZsh{}                             save\PYZus{}to\PYZus{}dir=kind\PYZus{}dir,}
        \PY{c+c1}{\PYZsh{}                             save\PYZus{}prefix=\PYZdq{}pic\PYZus{}plus\PYZdq{}, }
        \PY{c+c1}{\PYZsh{}                             save\PYZus{}format=\PYZdq{}jpg\PYZdq{})}
        \PY{c+c1}{\PYZsh{}         print (imageGen)}
        \PY{c+c1}{\PYZsh{}         for image in imageGen:}
        \PY{c+c1}{\PYZsh{}         \PYZsh{} increment our counter}
        \PY{c+c1}{\PYZsh{}             total += 1}
        \PY{c+c1}{\PYZsh{}             \PYZsh{} if we have reached 4 examples, break from the loop}
        \PY{c+c1}{\PYZsh{}             if total == 4:}
        \PY{c+c1}{\PYZsh{}                 break}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{train\PYZus{}dir}\PY{o}{=}\PY{n}{train\PYZus{}plus\PYZus{}dir}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications} \PY{k}{import} \PY{n}{resnet50}
        
        \PY{n}{conv\PYZus{}base} \PY{o}{=} \PY{n}{resnet50}\PY{o}{.}\PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                          \PY{n}{include\PYZus{}top}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                          \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
        \PY{n}{conv\PYZus{}base}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/xiaoming/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras\_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include\_top=False)` has been changed since Keras 2.2.0.
  warnings.warn('The output shape of `ResNet50(include\_top=False)` '

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                    Output Shape         Param \#     Connected to                     
==================================================================================================
input\_1 (InputLayer)            (None, 256, 256, 3)  0                                            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1\_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input\_1[0][0]                    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1 (Conv2D)                  (None, 128, 128, 64) 9472        conv1\_pad[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn\_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_1 (Activation)       (None, 128, 128, 64) 0           bn\_conv1[0][0]                   
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
pool1\_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           activation\_1[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_1 (MaxPooling2D)  (None, 64, 64, 64)   0           pool1\_pad[0][0]                  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2a\_branch2a (Conv2D)         (None, 64, 64, 64)   4160        max\_pooling2d\_1[0][0]            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2a\_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2a\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_2 (Activation)       (None, 64, 64, 64)   0           bn2a\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2a\_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation\_2[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2a\_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2a\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_3 (Activation)       (None, 64, 64, 64)   0           bn2a\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2a\_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation\_3[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2a\_branch1 (Conv2D)          (None, 64, 64, 256)  16640       max\_pooling2d\_1[0][0]            
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2a\_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2a\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2a\_branch1 (BatchNormalizatio (None, 64, 64, 256)  1024        res2a\_branch1[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_1 (Add)                     (None, 64, 64, 256)  0           bn2a\_branch2c[0][0]              
                                                                 bn2a\_branch1[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_4 (Activation)       (None, 64, 64, 256)  0           add\_1[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2b\_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation\_4[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2b\_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2b\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_5 (Activation)       (None, 64, 64, 64)   0           bn2b\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2b\_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation\_5[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2b\_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2b\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_6 (Activation)       (None, 64, 64, 64)   0           bn2b\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2b\_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation\_6[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2b\_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2b\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_2 (Add)                     (None, 64, 64, 256)  0           bn2b\_branch2c[0][0]              
                                                                 activation\_4[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_7 (Activation)       (None, 64, 64, 256)  0           add\_2[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2c\_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation\_7[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2c\_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2c\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_8 (Activation)       (None, 64, 64, 64)   0           bn2c\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2c\_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation\_8[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2c\_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2c\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_9 (Activation)       (None, 64, 64, 64)   0           bn2c\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res2c\_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation\_9[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn2c\_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2c\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_3 (Add)                     (None, 64, 64, 256)  0           bn2c\_branch2c[0][0]              
                                                                 activation\_7[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_10 (Activation)      (None, 64, 64, 256)  0           add\_3[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3a\_branch2a (Conv2D)         (None, 32, 32, 128)  32896       activation\_10[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3a\_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3a\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_11 (Activation)      (None, 32, 32, 128)  0           bn3a\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3a\_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation\_11[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3a\_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3a\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_12 (Activation)      (None, 32, 32, 128)  0           bn3a\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3a\_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation\_12[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3a\_branch1 (Conv2D)          (None, 32, 32, 512)  131584      activation\_10[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3a\_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3a\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3a\_branch1 (BatchNormalizatio (None, 32, 32, 512)  2048        res3a\_branch1[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_4 (Add)                     (None, 32, 32, 512)  0           bn3a\_branch2c[0][0]              
                                                                 bn3a\_branch1[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_13 (Activation)      (None, 32, 32, 512)  0           add\_4[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3b\_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation\_13[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3b\_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3b\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_14 (Activation)      (None, 32, 32, 128)  0           bn3b\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3b\_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation\_14[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3b\_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3b\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_15 (Activation)      (None, 32, 32, 128)  0           bn3b\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3b\_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation\_15[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3b\_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3b\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_5 (Add)                     (None, 32, 32, 512)  0           bn3b\_branch2c[0][0]              
                                                                 activation\_13[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_16 (Activation)      (None, 32, 32, 512)  0           add\_5[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3c\_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation\_16[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3c\_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3c\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_17 (Activation)      (None, 32, 32, 128)  0           bn3c\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3c\_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation\_17[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3c\_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3c\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_18 (Activation)      (None, 32, 32, 128)  0           bn3c\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3c\_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation\_18[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3c\_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3c\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_6 (Add)                     (None, 32, 32, 512)  0           bn3c\_branch2c[0][0]              
                                                                 activation\_16[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_19 (Activation)      (None, 32, 32, 512)  0           add\_6[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3d\_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation\_19[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3d\_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3d\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_20 (Activation)      (None, 32, 32, 128)  0           bn3d\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3d\_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation\_20[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3d\_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3d\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_21 (Activation)      (None, 32, 32, 128)  0           bn3d\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res3d\_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation\_21[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn3d\_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3d\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_7 (Add)                     (None, 32, 32, 512)  0           bn3d\_branch2c[0][0]              
                                                                 activation\_19[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_22 (Activation)      (None, 32, 32, 512)  0           add\_7[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4a\_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation\_22[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4a\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4a\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_23 (Activation)      (None, 16, 16, 256)  0           bn4a\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4a\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_23[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4a\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4a\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_24 (Activation)      (None, 16, 16, 256)  0           bn4a\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4a\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_24[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4a\_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      activation\_22[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4a\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4a\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4a\_branch1 (BatchNormalizatio (None, 16, 16, 1024) 4096        res4a\_branch1[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_8 (Add)                     (None, 16, 16, 1024) 0           bn4a\_branch2c[0][0]              
                                                                 bn4a\_branch1[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_25 (Activation)      (None, 16, 16, 1024) 0           add\_8[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4b\_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation\_25[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4b\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4b\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_26 (Activation)      (None, 16, 16, 256)  0           bn4b\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4b\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_26[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4b\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4b\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_27 (Activation)      (None, 16, 16, 256)  0           bn4b\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4b\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_27[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4b\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4b\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_9 (Add)                     (None, 16, 16, 1024) 0           bn4b\_branch2c[0][0]              
                                                                 activation\_25[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_28 (Activation)      (None, 16, 16, 1024) 0           add\_9[0][0]                      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4c\_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation\_28[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4c\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4c\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_29 (Activation)      (None, 16, 16, 256)  0           bn4c\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4c\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_29[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4c\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4c\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_30 (Activation)      (None, 16, 16, 256)  0           bn4c\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4c\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_30[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4c\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4c\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_10 (Add)                    (None, 16, 16, 1024) 0           bn4c\_branch2c[0][0]              
                                                                 activation\_28[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_31 (Activation)      (None, 16, 16, 1024) 0           add\_10[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4d\_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation\_31[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4d\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4d\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_32 (Activation)      (None, 16, 16, 256)  0           bn4d\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4d\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_32[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4d\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4d\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_33 (Activation)      (None, 16, 16, 256)  0           bn4d\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4d\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_33[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4d\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4d\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_11 (Add)                    (None, 16, 16, 1024) 0           bn4d\_branch2c[0][0]              
                                                                 activation\_31[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_34 (Activation)      (None, 16, 16, 1024) 0           add\_11[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4e\_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation\_34[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4e\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4e\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_35 (Activation)      (None, 16, 16, 256)  0           bn4e\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4e\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_35[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4e\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4e\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_36 (Activation)      (None, 16, 16, 256)  0           bn4e\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4e\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_36[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4e\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4e\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_12 (Add)                    (None, 16, 16, 1024) 0           bn4e\_branch2c[0][0]              
                                                                 activation\_34[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_37 (Activation)      (None, 16, 16, 1024) 0           add\_12[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4f\_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation\_37[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4f\_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4f\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_38 (Activation)      (None, 16, 16, 256)  0           bn4f\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4f\_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation\_38[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4f\_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4f\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_39 (Activation)      (None, 16, 16, 256)  0           bn4f\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res4f\_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation\_39[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn4f\_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4f\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_13 (Add)                    (None, 16, 16, 1024) 0           bn4f\_branch2c[0][0]              
                                                                 activation\_37[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_40 (Activation)      (None, 16, 16, 1024) 0           add\_13[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5a\_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation\_40[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5a\_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5a\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_41 (Activation)      (None, 8, 8, 512)    0           bn5a\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5a\_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation\_41[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5a\_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5a\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_42 (Activation)      (None, 8, 8, 512)    0           bn5a\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5a\_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation\_42[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5a\_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     activation\_40[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5a\_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5a\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5a\_branch1 (BatchNormalizatio (None, 8, 8, 2048)   8192        res5a\_branch1[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_14 (Add)                    (None, 8, 8, 2048)   0           bn5a\_branch2c[0][0]              
                                                                 bn5a\_branch1[0][0]               
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_43 (Activation)      (None, 8, 8, 2048)   0           add\_14[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5b\_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation\_43[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5b\_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5b\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_44 (Activation)      (None, 8, 8, 512)    0           bn5b\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5b\_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation\_44[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5b\_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5b\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_45 (Activation)      (None, 8, 8, 512)    0           bn5b\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5b\_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation\_45[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5b\_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5b\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_15 (Add)                    (None, 8, 8, 2048)   0           bn5b\_branch2c[0][0]              
                                                                 activation\_43[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_46 (Activation)      (None, 8, 8, 2048)   0           add\_15[0][0]                     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5c\_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation\_46[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5c\_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5c\_branch2a[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_47 (Activation)      (None, 8, 8, 512)    0           bn5c\_branch2a[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5c\_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation\_47[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5c\_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5c\_branch2b[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_48 (Activation)      (None, 8, 8, 512)    0           bn5c\_branch2b[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
res5c\_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation\_48[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
bn5c\_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5c\_branch2c[0][0]             
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
add\_16 (Add)                    (None, 8, 8, 2048)   0           bn5c\_branch2c[0][0]              
                                                                 activation\_46[0][0]              
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
activation\_49 (Activation)      (None, 8, 8, 2048)   0           add\_16[0][0]                     
==================================================================================================
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{layers}
        \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{models}
        
        \PY{n}{model} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{conv\PYZus{}base}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{layers}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
resnet50 (Model)             (None, 8, 8, 2048)        23587712  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_1 (Flatten)          (None, 131072)            0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 133)               17432709  
=================================================================
Total params: 41,020,421
Trainable params: 40,967,301
Non-trainable params: 53,120
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{}conv\PYZus{}base.trainable = False}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{optimizers}
        \PY{c+c1}{\PYZsh{} lr: float \PYZgt{}= 0. 学习率 Learning rate}
        
        \PY{c+c1}{\PYZsh{} momentum: float \PYZgt{}= 0. 参数更新动量 parameter updates momentum}
        
        \PY{c+c1}{\PYZsh{} decay: float \PYZgt{}= 0. 学习率每次更新的下降率 Learning rate decay over each update}
        
        \PY{c+c1}{\PYZsh{} nesterov: boolean. 是否应用 Nesterov 动量 whether to apply Nesterov momentum}
        \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{n}{optimizer}\PY{o}{=}\PY{n}{optimizers}\PY{o}{.}\PY{n}{RMSprop}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{,}
                      \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{ImageDataGenerator}
        
        \PY{c+c1}{\PYZsh{} All images will be rescaled by 1./255}
        \PY{n}{train\PYZus{}datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{rescale}\PY{o}{=}\PY{l+m+mf}{1.}\PY{o}{/}\PY{l+m+mi}{255}\PY{p}{)}
        \PY{n}{validation\PYZus{}datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{rescale}\PY{o}{=}\PY{l+m+mf}{1.}\PY{o}{/}\PY{l+m+mi}{255}\PY{p}{)}
        
        \PY{n}{train\PYZus{}generator} \PY{o}{=} \PY{n}{train\PYZus{}datagen}\PY{o}{.}\PY{n}{flow\PYZus{}from\PYZus{}directory}\PY{p}{(}
                \PY{c+c1}{\PYZsh{} This is the target directory}
                \PY{n}{train\PYZus{}dir}\PY{p}{,}
                \PY{c+c1}{\PYZsh{} All images will be resized to 150x150}
                \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{)}\PY{p}{,}
                \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
                \PY{c+c1}{\PYZsh{} Since we use binary\PYZus{}crossentropy loss, we need binary labels}
                \PY{n}{class\PYZus{}mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{validation\PYZus{}generator} \PY{o}{=} \PY{n}{validation\PYZus{}datagen}\PY{o}{.}\PY{n}{flow\PYZus{}from\PYZus{}directory}\PY{p}{(}
                \PY{n}{validation\PYZus{}dir}\PY{p}{,}
                \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{)}\PY{p}{,}
                \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                \PY{n}{class\PYZus{}mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{validation\PYZus{}generator}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{validation\PYZus{}generator}\PY{o}{.}\PY{n}{class\PYZus{}indices}\PY{p}{)}
        \PY{n}{np}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels.npy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{validation\PYZus{}generator}\PY{o}{.}\PY{n}{class\PYZus{}indices}\PY{p}{)} 
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Found 20417 images belonging to 133 classes.
Found 835 images belonging to 133 classes.
\{'001.Affenpinscher': 0, '002.Afghan\_hound': 1, '003.Airedale\_terrier': 2, '004.Akita': 3, '005.Alaskan\_malamute': 4, '006.American\_eskimo\_dog': 5, '007.American\_foxhound': 6, '008.American\_staffordshire\_terrier': 7, '009.American\_water\_spaniel': 8, '010.Anatolian\_shepherd\_dog': 9, '011.Australian\_cattle\_dog': 10, '012.Australian\_shepherd': 11, '013.Australian\_terrier': 12, '014.Basenji': 13, '015.Basset\_hound': 14, '016.Beagle': 15, '017.Bearded\_collie': 16, '018.Beauceron': 17, '019.Bedlington\_terrier': 18, '020.Belgian\_malinois': 19, '021.Belgian\_sheepdog': 20, '022.Belgian\_tervuren': 21, '023.Bernese\_mountain\_dog': 22, '024.Bichon\_frise': 23, '025.Black\_and\_tan\_coonhound': 24, '026.Black\_russian\_terrier': 25, '027.Bloodhound': 26, '028.Bluetick\_coonhound': 27, '029.Border\_collie': 28, '030.Border\_terrier': 29, '031.Borzoi': 30, '032.Boston\_terrier': 31, '033.Bouvier\_des\_flandres': 32, '034.Boxer': 33, '035.Boykin\_spaniel': 34, '036.Briard': 35, '037.Brittany': 36, '038.Brussels\_griffon': 37, '039.Bull\_terrier': 38, '040.Bulldog': 39, '041.Bullmastiff': 40, '042.Cairn\_terrier': 41, '043.Canaan\_dog': 42, '044.Cane\_corso': 43, '045.Cardigan\_welsh\_corgi': 44, '046.Cavalier\_king\_charles\_spaniel': 45, '047.Chesapeake\_bay\_retriever': 46, '048.Chihuahua': 47, '049.Chinese\_crested': 48, '050.Chinese\_shar-pei': 49, '051.Chow\_chow': 50, '052.Clumber\_spaniel': 51, '053.Cocker\_spaniel': 52, '054.Collie': 53, '055.Curly-coated\_retriever': 54, '056.Dachshund': 55, '057.Dalmatian': 56, '058.Dandie\_dinmont\_terrier': 57, '059.Doberman\_pinscher': 58, '060.Dogue\_de\_bordeaux': 59, '061.English\_cocker\_spaniel': 60, '062.English\_setter': 61, '063.English\_springer\_spaniel': 62, '064.English\_toy\_spaniel': 63, '065.Entlebucher\_mountain\_dog': 64, '066.Field\_spaniel': 65, '067.Finnish\_spitz': 66, '068.Flat-coated\_retriever': 67, '069.French\_bulldog': 68, '070.German\_pinscher': 69, '071.German\_shepherd\_dog': 70, '072.German\_shorthaired\_pointer': 71, '073.German\_wirehaired\_pointer': 72, '074.Giant\_schnauzer': 73, '075.Glen\_of\_imaal\_terrier': 74, '076.Golden\_retriever': 75, '077.Gordon\_setter': 76, '078.Great\_dane': 77, '079.Great\_pyrenees': 78, '080.Greater\_swiss\_mountain\_dog': 79, '081.Greyhound': 80, '082.Havanese': 81, '083.Ibizan\_hound': 82, '084.Icelandic\_sheepdog': 83, '085.Irish\_red\_and\_white\_setter': 84, '086.Irish\_setter': 85, '087.Irish\_terrier': 86, '088.Irish\_water\_spaniel': 87, '089.Irish\_wolfhound': 88, '090.Italian\_greyhound': 89, '091.Japanese\_chin': 90, '092.Keeshond': 91, '093.Kerry\_blue\_terrier': 92, '094.Komondor': 93, '095.Kuvasz': 94, '096.Labrador\_retriever': 95, '097.Lakeland\_terrier': 96, '098.Leonberger': 97, '099.Lhasa\_apso': 98, '100.Lowchen': 99, '101.Maltese': 100, '102.Manchester\_terrier': 101, '103.Mastiff': 102, '104.Miniature\_schnauzer': 103, '105.Neapolitan\_mastiff': 104, '106.Newfoundland': 105, '107.Norfolk\_terrier': 106, '108.Norwegian\_buhund': 107, '109.Norwegian\_elkhound': 108, '110.Norwegian\_lundehund': 109, '111.Norwich\_terrier': 110, '112.Nova\_scotia\_duck\_tolling\_retriever': 111, '113.Old\_english\_sheepdog': 112, '114.Otterhound': 113, '115.Papillon': 114, '116.Parson\_russell\_terrier': 115, '117.Pekingese': 116, '118.Pembroke\_welsh\_corgi': 117, '119.Petit\_basset\_griffon\_vendeen': 118, '120.Pharaoh\_hound': 119, '121.Plott': 120, '122.Pointer': 121, '123.Pomeranian': 122, '124.Poodle': 123, '125.Portuguese\_water\_dog': 124, '126.Saint\_bernard': 125, '127.Silky\_terrier': 126, '128.Smooth\_fox\_terrier': 127, '129.Tibetan\_mastiff': 128, '130.Welsh\_springer\_spaniel': 129, '131.Wirehaired\_pointing\_griffon': 130, '132.Xoloitzcuintli': 131, '133.Yorkshire\_terrier': 132\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{for} \PY{n}{data\PYZus{}batch}\PY{p}{,} \PY{n}{labels\PYZus{}batch} \PY{o+ow}{in} \PY{n}{train\PYZus{}generator}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data batch shape:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data\PYZus{}batch}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labels batch shape:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{labels\PYZus{}batch}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{k}{break}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
data batch shape: (20, 256, 256, 3)
labels batch shape: (20, 133)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{EarlyStopping}
         \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{ImageFile}
         \PY{n}{early\PYZus{}stopping} \PY{o}{=} \PY{n}{EarlyStopping}\PY{p}{(}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{7}\PY{p}{)} 
         \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogskindresnet.model.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} 
                                        \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}generator}\PY{p}{(}\PY{n}{train\PYZus{}generator}\PY{p}{,}
               \PY{n}{steps\PYZus{}per\PYZus{}epoch}\PY{o}{=}\PY{l+m+mi}{400}\PY{p}{,}
               \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
               \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{validation\PYZus{}generator}\PY{p}{,}
               \PY{n}{validation\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} 
               \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{early\PYZus{}stopping}\PY{p}{,}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} 
               \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/100
400/400 [==============================] - 142s 354ms/step - loss: 6.4893 - acc: 0.0151 - val\_loss: 5.8483 - val\_acc: 0.0229

Epoch 00001: val\_loss improved from inf to 5.84834, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 2/100
400/400 [==============================] - 131s 329ms/step - loss: 5.4670 - acc: 0.0479 - val\_loss: 4.9366 - val\_acc: 0.0694

Epoch 00002: val\_loss improved from 5.84834 to 4.93662, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 3/100
400/400 [==============================] - 132s 330ms/step - loss: 4.3984 - acc: 0.1310 - val\_loss: 4.2053 - val\_acc: 0.1425

Epoch 00003: val\_loss improved from 4.93662 to 4.20526, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 4/100
400/400 [==============================] - 132s 329ms/step - loss: 3.5543 - acc: 0.2176 - val\_loss: 3.5782 - val\_acc: 0.2195

Epoch 00004: val\_loss improved from 4.20526 to 3.57820, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 5/100
400/400 [==============================] - 123s 308ms/step - loss: 3.0298 - acc: 0.2878 - val\_loss: 3.1240 - val\_acc: 0.2879

Epoch 00005: val\_loss improved from 3.57820 to 3.12398, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 6/100
400/400 [==============================] - 122s 304ms/step - loss: 2.3648 - acc: 0.4033 - val\_loss: 2.7410 - val\_acc: 0.3432

Epoch 00006: val\_loss improved from 3.12398 to 2.74103, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 7/100
400/400 [==============================] - 121s 303ms/step - loss: 2.0107 - acc: 0.4756 - val\_loss: 2.4928 - val\_acc: 0.3799

Epoch 00007: val\_loss improved from 2.74103 to 2.49284, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 8/100
400/400 [==============================] - 122s 305ms/step - loss: 1.7118 - acc: 0.5380 - val\_loss: 2.2403 - val\_acc: 0.4390

Epoch 00008: val\_loss improved from 2.49284 to 2.24029, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 9/100
400/400 [==============================] - 121s 302ms/step - loss: 1.3781 - acc: 0.6128 - val\_loss: 2.0893 - val\_acc: 0.4675

Epoch 00009: val\_loss improved from 2.24029 to 2.08929, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 10/100
400/400 [==============================] - 121s 302ms/step - loss: 1.2613 - acc: 0.6389 - val\_loss: 1.9419 - val\_acc: 0.4995

Epoch 00010: val\_loss improved from 2.08929 to 1.94190, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 11/100
400/400 [==============================] - 121s 303ms/step - loss: 1.0020 - acc: 0.7077 - val\_loss: 1.8177 - val\_acc: 0.5069

Epoch 00011: val\_loss improved from 1.94190 to 1.81770, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 12/100
400/400 [==============================] - 123s 307ms/step - loss: 0.8994 - acc: 0.7321 - val\_loss: 1.7259 - val\_acc: 0.5298

Epoch 00012: val\_loss improved from 1.81770 to 1.72588, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 13/100
400/400 [==============================] - 120s 301ms/step - loss: 0.8080 - acc: 0.7664 - val\_loss: 1.6692 - val\_acc: 0.5453

Epoch 00013: val\_loss improved from 1.72588 to 1.66916, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 14/100
400/400 [==============================] - 121s 303ms/step - loss: 0.6489 - acc: 0.8103 - val\_loss: 1.5823 - val\_acc: 0.5591

Epoch 00014: val\_loss improved from 1.66916 to 1.58231, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 15/100
400/400 [==============================] - 122s 304ms/step - loss: 0.6283 - acc: 0.8150 - val\_loss: 1.5478 - val\_acc: 0.5705

Epoch 00015: val\_loss improved from 1.58231 to 1.54784, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 16/100
400/400 [==============================] - 121s 302ms/step - loss: 0.5123 - acc: 0.8496 - val\_loss: 1.4762 - val\_acc: 0.5771

Epoch 00016: val\_loss improved from 1.54784 to 1.47616, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 17/100
400/400 [==============================] - 121s 303ms/step - loss: 0.4445 - acc: 0.8731 - val\_loss: 1.4480 - val\_acc: 0.5866

Epoch 00017: val\_loss improved from 1.47616 to 1.44803, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 18/100
400/400 [==============================] - 121s 302ms/step - loss: 0.4151 - acc: 0.8787 - val\_loss: 1.4058 - val\_acc: 0.6004

Epoch 00018: val\_loss improved from 1.44803 to 1.40585, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 19/100
400/400 [==============================] - 122s 305ms/step - loss: 0.3293 - acc: 0.9087 - val\_loss: 1.3650 - val\_acc: 0.5996

Epoch 00019: val\_loss improved from 1.40585 to 1.36503, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 20/100
400/400 [==============================] - 121s 301ms/step - loss: 0.2967 - acc: 0.9136 - val\_loss: 1.3416 - val\_acc: 0.6130

Epoch 00020: val\_loss improved from 1.36503 to 1.34157, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 21/100
400/400 [==============================] - 122s 305ms/step - loss: 0.2529 - acc: 0.9327 - val\_loss: 1.3087 - val\_acc: 0.6169

Epoch 00021: val\_loss improved from 1.34157 to 1.30865, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 22/100
400/400 [==============================] - 120s 300ms/step - loss: 0.2302 - acc: 0.9380 - val\_loss: 1.2961 - val\_acc: 0.6243

Epoch 00022: val\_loss improved from 1.30865 to 1.29611, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 23/100
400/400 [==============================] - 121s 303ms/step - loss: 0.2083 - acc: 0.9455 - val\_loss: 1.2622 - val\_acc: 0.6344

Epoch 00023: val\_loss improved from 1.29611 to 1.26215, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 24/100
400/400 [==============================] - 120s 299ms/step - loss: 0.1541 - acc: 0.9647 - val\_loss: 1.2738 - val\_acc: 0.6362

Epoch 00024: val\_loss did not improve from 1.26215
Epoch 25/100
400/400 [==============================] - 121s 302ms/step - loss: 0.1560 - acc: 0.9635 - val\_loss: 1.2181 - val\_acc: 0.6417

Epoch 00025: val\_loss improved from 1.26215 to 1.21807, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 26/100
400/400 [==============================] - 120s 300ms/step - loss: 0.1278 - acc: 0.9706 - val\_loss: 1.2250 - val\_acc: 0.6512

Epoch 00026: val\_loss did not improve from 1.21807
Epoch 27/100
400/400 [==============================] - 119s 297ms/step - loss: 0.1049 - acc: 0.9785 - val\_loss: 1.2000 - val\_acc: 0.6534

Epoch 00027: val\_loss improved from 1.21807 to 1.19996, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 28/100
400/400 [==============================] - 122s 306ms/step - loss: 0.1032 - acc: 0.9786 - val\_loss: 1.1914 - val\_acc: 0.6539

Epoch 00028: val\_loss improved from 1.19996 to 1.19142, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 29/100
400/400 [==============================] - 121s 302ms/step - loss: 0.0763 - acc: 0.9860 - val\_loss: 1.1841 - val\_acc: 0.6625

Epoch 00029: val\_loss improved from 1.19142 to 1.18412, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 30/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0713 - acc: 0.9869 - val\_loss: 1.1617 - val\_acc: 0.6663

Epoch 00030: val\_loss improved from 1.18412 to 1.16171, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 31/100
400/400 [==============================] - 122s 306ms/step - loss: 0.0671 - acc: 0.9872 - val\_loss: 1.1419 - val\_acc: 0.6720

Epoch 00031: val\_loss improved from 1.16171 to 1.14190, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 32/100
400/400 [==============================] - 121s 304ms/step - loss: 0.0525 - acc: 0.9915 - val\_loss: 1.1573 - val\_acc: 0.6697

Epoch 00032: val\_loss did not improve from 1.14190
Epoch 33/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0490 - acc: 0.9934 - val\_loss: 1.1264 - val\_acc: 0.6703

Epoch 00033: val\_loss improved from 1.14190 to 1.12640, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 34/100
400/400 [==============================] - 122s 306ms/step - loss: 0.0370 - acc: 0.9949 - val\_loss: 1.1370 - val\_acc: 0.6747

Epoch 00034: val\_loss did not improve from 1.12640
Epoch 35/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0353 - acc: 0.9950 - val\_loss: 1.1167 - val\_acc: 0.6861

Epoch 00035: val\_loss improved from 1.12640 to 1.11669, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 36/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0303 - acc: 0.9969 - val\_loss: 1.1274 - val\_acc: 0.6770

Epoch 00036: val\_loss did not improve from 1.11669
Epoch 37/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0257 - acc: 0.9977 - val\_loss: 1.1235 - val\_acc: 0.6877

Epoch 00037: val\_loss did not improve from 1.11669
Epoch 38/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0215 - acc: 0.9987 - val\_loss: 1.0960 - val\_acc: 0.6883

Epoch 00038: val\_loss improved from 1.11669 to 1.09596, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 39/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0182 - acc: 0.9987 - val\_loss: 1.0926 - val\_acc: 0.6879

Epoch 00039: val\_loss improved from 1.09596 to 1.09263, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 40/100
400/400 [==============================] - 123s 308ms/step - loss: 0.0179 - acc: 0.9991 - val\_loss: 1.1134 - val\_acc: 0.6870

Epoch 00040: val\_loss did not improve from 1.09263
Epoch 41/100
400/400 [==============================] - 118s 296ms/step - loss: 0.0165 - acc: 0.9984 - val\_loss: 1.0909 - val\_acc: 0.6935

Epoch 00041: val\_loss improved from 1.09263 to 1.09095, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 42/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0123 - acc: 0.9992 - val\_loss: 1.0835 - val\_acc: 0.6961

Epoch 00042: val\_loss improved from 1.09095 to 1.08355, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 43/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0117 - acc: 0.9990 - val\_loss: 1.0617 - val\_acc: 0.6997

Epoch 00043: val\_loss improved from 1.08355 to 1.06171, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 44/100
400/400 [==============================] - 121s 304ms/step - loss: 0.0096 - acc: 0.9995 - val\_loss: 1.0679 - val\_acc: 0.6961

Epoch 00044: val\_loss did not improve from 1.06171
Epoch 45/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0093 - acc: 0.9991 - val\_loss: 1.0761 - val\_acc: 0.7048

Epoch 00045: val\_loss did not improve from 1.06171
Epoch 46/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0085 - acc: 0.9991 - val\_loss: 1.0581 - val\_acc: 0.7014

Epoch 00046: val\_loss improved from 1.06171 to 1.05809, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 47/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0070 - acc: 0.9994 - val\_loss: 1.0694 - val\_acc: 0.7015

Epoch 00047: val\_loss did not improve from 1.05809
Epoch 48/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0063 - acc: 0.9995 - val\_loss: 1.0753 - val\_acc: 0.7016

Epoch 00048: val\_loss did not improve from 1.05809
Epoch 49/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0052 - acc: 0.9997 - val\_loss: 1.0497 - val\_acc: 0.7080

Epoch 00049: val\_loss improved from 1.05809 to 1.04973, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 50/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0048 - acc: 0.9997 - val\_loss: 1.0539 - val\_acc: 0.7099

Epoch 00050: val\_loss did not improve from 1.04973
Epoch 51/100
400/400 [==============================] - 122s 304ms/step - loss: 0.0047 - acc: 0.9994 - val\_loss: 1.0423 - val\_acc: 0.7115

Epoch 00051: val\_loss improved from 1.04973 to 1.04232, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 52/100
400/400 [==============================] - 120s 301ms/step - loss: 0.0038 - acc: 0.9995 - val\_loss: 1.0603 - val\_acc: 0.7153

Epoch 00052: val\_loss did not improve from 1.04232
Epoch 53/100
400/400 [==============================] - 120s 300ms/step - loss: 0.0038 - acc: 0.9999 - val\_loss: 1.0532 - val\_acc: 0.7215

Epoch 00053: val\_loss did not improve from 1.04232
Epoch 54/100
400/400 [==============================] - 119s 299ms/step - loss: 0.0028 - acc: 0.9997 - val\_loss: 1.0482 - val\_acc: 0.7145

Epoch 00054: val\_loss did not improve from 1.04232
Epoch 55/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0025 - acc: 0.9999 - val\_loss: 1.0477 - val\_acc: 0.7179

Epoch 00055: val\_loss did not improve from 1.04232
Epoch 56/100
400/400 [==============================] - 121s 302ms/step - loss: 0.0030 - acc: 0.9995 - val\_loss: 1.0408 - val\_acc: 0.7217

Epoch 00056: val\_loss improved from 1.04232 to 1.04083, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 57/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0025 - acc: 0.9995 - val\_loss: 1.0340 - val\_acc: 0.7172

Epoch 00057: val\_loss improved from 1.04083 to 1.03403, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 58/100
400/400 [==============================] - 125s 314ms/step - loss: 0.0021 - acc: 0.9997 - val\_loss: 1.0438 - val\_acc: 0.7187

Epoch 00058: val\_loss did not improve from 1.03403
Epoch 59/100
400/400 [==============================] - 120s 300ms/step - loss: 0.0033 - acc: 0.9992 - val\_loss: 1.0336 - val\_acc: 0.7260

Epoch 00059: val\_loss improved from 1.03403 to 1.03364, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 60/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0027 - acc: 0.9994 - val\_loss: 1.0505 - val\_acc: 0.7180

Epoch 00060: val\_loss did not improve from 1.03364
Epoch 61/100
400/400 [==============================] - 121s 302ms/step - loss: 0.0023 - acc: 0.9997 - val\_loss: 1.0441 - val\_acc: 0.7264

Epoch 00061: val\_loss did not improve from 1.03364
Epoch 62/100
400/400 [==============================] - 122s 305ms/step - loss: 0.0026 - acc: 0.9994 - val\_loss: 1.0332 - val\_acc: 0.7297

Epoch 00062: val\_loss improved from 1.03364 to 1.03322, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 63/100
400/400 [==============================] - 121s 301ms/step - loss: 0.0012 - acc: 0.9999 - val\_loss: 1.0360 - val\_acc: 0.7336

Epoch 00063: val\_loss did not improve from 1.03322
Epoch 64/100
400/400 [==============================] - 124s 309ms/step - loss: 0.0016 - acc: 0.9997 - val\_loss: 1.0330 - val\_acc: 0.7360

Epoch 00064: val\_loss improved from 1.03322 to 1.03303, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 65/100
400/400 [==============================] - 121s 301ms/step - loss: 0.0017 - acc: 0.9996 - val\_loss: 1.0362 - val\_acc: 0.7241

Epoch 00065: val\_loss did not improve from 1.03303
Epoch 66/100
400/400 [==============================] - 122s 306ms/step - loss: 0.0017 - acc: 0.9996 - val\_loss: 1.0402 - val\_acc: 0.7348

Epoch 00066: val\_loss did not improve from 1.03303
Epoch 67/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0011 - acc: 0.9997 - val\_loss: 1.0200 - val\_acc: 0.7293

Epoch 00067: val\_loss improved from 1.03303 to 1.02001, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 68/100
400/400 [==============================] - 122s 306ms/step - loss: 0.0014 - acc: 0.9995 - val\_loss: 1.0333 - val\_acc: 0.7320

Epoch 00068: val\_loss did not improve from 1.02001
Epoch 69/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0012 - acc: 0.9996 - val\_loss: 1.0326 - val\_acc: 0.7351

Epoch 00069: val\_loss did not improve from 1.02001
Epoch 70/100
400/400 [==============================] - 123s 307ms/step - loss: 0.0012 - acc: 0.9997 - val\_loss: 1.0464 - val\_acc: 0.7358

Epoch 00070: val\_loss did not improve from 1.02001
Epoch 71/100
400/400 [==============================] - 121s 303ms/step - loss: 0.0019 - acc: 0.9994 - val\_loss: 1.0334 - val\_acc: 0.7302

Epoch 00071: val\_loss did not improve from 1.02001
Epoch 72/100
400/400 [==============================] - 122s 304ms/step - loss: 0.0013 - acc: 0.9996 - val\_loss: 1.0201 - val\_acc: 0.7462

Epoch 00072: val\_loss did not improve from 1.02001
Epoch 73/100
400/400 [==============================] - 122s 305ms/step - loss: 9.4509e-04 - acc: 0.9997 - val\_loss: 1.0412 - val\_acc: 0.7360

Epoch 00073: val\_loss did not improve from 1.02001
Epoch 74/100
400/400 [==============================] - 122s 304ms/step - loss: 0.0011 - acc: 0.9994 - val\_loss: 1.0291 - val\_acc: 0.7373

Epoch 00074: val\_loss did not improve from 1.02001

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{early\PYZus{}stopping} \PY{o}{=} \PY{n}{EarlyStopping}\PY{p}{(}\PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{35}\PY{p}{)} 
         \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogskindresnet.model.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} 
                                        \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{history} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}generator}\PY{p}{(}\PY{n}{train\PYZus{}generator}\PY{p}{,}
               \PY{n}{steps\PYZus{}per\PYZus{}epoch}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
               \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,}
               \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{n}{validation\PYZus{}generator}\PY{p}{,}
               \PY{n}{validation\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} 
               \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{early\PYZus{}stopping}\PY{p}{,}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} 
               \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/10000
100/100 [==============================] - 61s 615ms/step - loss: 6.6231e-04 - acc: 1.0000 - val\_loss: 1.0412 - val\_acc: 0.7358

Epoch 00001: val\_loss improved from inf to 1.04116, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 2/10000
100/100 [==============================] - 61s 611ms/step - loss: 5.4980e-04 - acc: 1.0000 - val\_loss: 1.0325 - val\_acc: 0.7306

Epoch 00002: val\_loss improved from 1.04116 to 1.03248, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 3/10000
100/100 [==============================] - 65s 646ms/step - loss: 9.3114e-04 - acc: 0.9995 - val\_loss: 1.0369 - val\_acc: 0.7363

Epoch 00003: val\_loss did not improve from 1.03248
Epoch 4/10000
100/100 [==============================] - 64s 638ms/step - loss: 0.0021 - acc: 0.9995 - val\_loss: 1.0167 - val\_acc: 0.7343

Epoch 00004: val\_loss improved from 1.03248 to 1.01674, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 5/10000
100/100 [==============================] - 63s 633ms/step - loss: 0.0017 - acc: 0.9995 - val\_loss: 1.0293 - val\_acc: 0.7367

Epoch 00005: val\_loss did not improve from 1.01674
Epoch 6/10000
100/100 [==============================] - 61s 608ms/step - loss: 0.0013 - acc: 0.9995 - val\_loss: 1.0268 - val\_acc: 0.7335

Epoch 00006: val\_loss did not improve from 1.01674
Epoch 7/10000
100/100 [==============================] - 59s 587ms/step - loss: 0.0026 - acc: 0.9995 - val\_loss: 1.0286 - val\_acc: 0.7371

Epoch 00007: val\_loss did not improve from 1.01674
Epoch 8/10000
100/100 [==============================] - 60s 603ms/step - loss: 0.0022 - acc: 0.9995 - val\_loss: 1.0351 - val\_acc: 0.7337

Epoch 00008: val\_loss did not improve from 1.01674
Epoch 9/10000
100/100 [==============================] - 61s 613ms/step - loss: 0.0017 - acc: 0.9995 - val\_loss: 1.0304 - val\_acc: 0.7343

Epoch 00009: val\_loss did not improve from 1.01674
Epoch 10/10000
100/100 [==============================] - 60s 596ms/step - loss: 7.9990e-04 - acc: 1.0000 - val\_loss: 1.0216 - val\_acc: 0.7397

Epoch 00010: val\_loss did not improve from 1.01674
Epoch 11/10000
100/100 [==============================] - 59s 593ms/step - loss: 8.6400e-04 - acc: 1.0000 - val\_loss: 1.0300 - val\_acc: 0.7377

Epoch 00011: val\_loss did not improve from 1.01674
Epoch 12/10000
100/100 [==============================] - 62s 620ms/step - loss: 0.0026 - acc: 0.9995 - val\_loss: 1.0403 - val\_acc: 0.7318

Epoch 00012: val\_loss did not improve from 1.01674
Epoch 13/10000
100/100 [==============================] - 61s 607ms/step - loss: 0.0025 - acc: 0.9995 - val\_loss: 1.0364 - val\_acc: 0.7317

Epoch 00013: val\_loss did not improve from 1.01674
Epoch 14/10000
100/100 [==============================] - 59s 595ms/step - loss: 5.7353e-04 - acc: 1.0000 - val\_loss: 1.0374 - val\_acc: 0.7360

Epoch 00014: val\_loss did not improve from 1.01674
Epoch 15/10000
100/100 [==============================] - 60s 604ms/step - loss: 3.8424e-04 - acc: 1.0000 - val\_loss: 1.0227 - val\_acc: 0.7446

Epoch 00015: val\_loss did not improve from 1.01674
Epoch 16/10000
100/100 [==============================] - 59s 592ms/step - loss: 5.3259e-04 - acc: 1.0000 - val\_loss: 1.0435 - val\_acc: 0.7390

Epoch 00016: val\_loss did not improve from 1.01674
Epoch 17/10000
100/100 [==============================] - 61s 609ms/step - loss: 3.4164e-04 - acc: 1.0000 - val\_loss: 1.0396 - val\_acc: 0.7360

Epoch 00017: val\_loss did not improve from 1.01674
Epoch 18/10000
100/100 [==============================] - 59s 590ms/step - loss: 7.2334e-04 - acc: 1.0000 - val\_loss: 1.0441 - val\_acc: 0.7362

Epoch 00018: val\_loss did not improve from 1.01674
Epoch 19/10000
100/100 [==============================] - 60s 604ms/step - loss: 0.0039 - acc: 0.9990 - val\_loss: 1.0404 - val\_acc: 0.7403

Epoch 00019: val\_loss did not improve from 1.01674
Epoch 20/10000
100/100 [==============================] - 60s 597ms/step - loss: 8.1641e-04 - acc: 0.9995 - val\_loss: 1.0128 - val\_acc: 0.7421

Epoch 00020: val\_loss improved from 1.01674 to 1.01281, saving model to dogskindresnet.model.weights.best.hdf5
Epoch 21/10000
100/100 [==============================] - 61s 609ms/step - loss: 8.2937e-04 - acc: 0.9995 - val\_loss: 1.0427 - val\_acc: 0.7352

Epoch 00021: val\_loss did not improve from 1.01281
Epoch 22/10000
100/100 [==============================] - 60s 599ms/step - loss: 0.0023 - acc: 0.9990 - val\_loss: 1.0323 - val\_acc: 0.7341

Epoch 00022: val\_loss did not improve from 1.01281
Epoch 23/10000
100/100 [==============================] - 59s 586ms/step - loss: 3.2099e-04 - acc: 1.0000 - val\_loss: 1.0168 - val\_acc: 0.7395

Epoch 00023: val\_loss did not improve from 1.01281
Epoch 24/10000
100/100 [==============================] - 59s 587ms/step - loss: 0.0019 - acc: 0.9995 - val\_loss: 1.0268 - val\_acc: 0.7341

Epoch 00024: val\_loss did not improve from 1.01281
Epoch 25/10000
100/100 [==============================] - 60s 599ms/step - loss: 3.9310e-04 - acc: 1.0000 - val\_loss: 1.0374 - val\_acc: 0.7393

Epoch 00025: val\_loss did not improve from 1.01281
Epoch 26/10000
100/100 [==============================] - 62s 622ms/step - loss: 0.0022 - acc: 0.9995 - val\_loss: 1.0376 - val\_acc: 0.7341

Epoch 00026: val\_loss did not improve from 1.01281
Epoch 27/10000
100/100 [==============================] - 60s 603ms/step - loss: 0.0025 - acc: 0.9995 - val\_loss: 1.0223 - val\_acc: 0.7410

Epoch 00027: val\_loss did not improve from 1.01281
Epoch 28/10000
100/100 [==============================] - 60s 595ms/step - loss: 4.1228e-04 - acc: 1.0000 - val\_loss: 1.0385 - val\_acc: 0.7393

Epoch 00028: val\_loss did not improve from 1.01281
Epoch 29/10000
100/100 [==============================] - 62s 621ms/step - loss: 3.0566e-04 - acc: 1.0000 - val\_loss: 1.0236 - val\_acc: 0.7375

Epoch 00029: val\_loss did not improve from 1.01281
Epoch 30/10000
100/100 [==============================] - 62s 619ms/step - loss: 2.7595e-04 - acc: 1.0000 - val\_loss: 1.0456 - val\_acc: 0.7382

Epoch 00030: val\_loss did not improve from 1.01281
Epoch 31/10000
100/100 [==============================] - 62s 616ms/step - loss: 0.0018 - acc: 0.9995 - val\_loss: 1.0367 - val\_acc: 0.7423

Epoch 00031: val\_loss did not improve from 1.01281
Epoch 32/10000
100/100 [==============================] - 61s 613ms/step - loss: 3.3485e-04 - acc: 1.0000 - val\_loss: 1.0412 - val\_acc: 0.7390

Epoch 00032: val\_loss did not improve from 1.01281
Epoch 33/10000
100/100 [==============================] - 61s 614ms/step - loss: 6.1598e-04 - acc: 1.0000 - val\_loss: 1.0387 - val\_acc: 0.7380

Epoch 00033: val\_loss did not improve from 1.01281
Epoch 34/10000
100/100 [==============================] - 61s 612ms/step - loss: 4.4849e-04 - acc: 1.0000 - val\_loss: 1.0372 - val\_acc: 0.7373

Epoch 00034: val\_loss did not improve from 1.01281
Epoch 35/10000
100/100 [==============================] - 62s 625ms/step - loss: 2.9831e-04 - acc: 1.0000 - val\_loss: 1.0342 - val\_acc: 0.7403

Epoch 00035: val\_loss did not improve from 1.01281
Epoch 36/10000
100/100 [==============================] - 62s 615ms/step - loss: 0.0024 - acc: 0.9995 - val\_loss: 1.0411 - val\_acc: 0.7414

Epoch 00036: val\_loss did not improve from 1.01281
Epoch 37/10000
100/100 [==============================] - 62s 624ms/step - loss: 0.0028 - acc: 0.9995 - val\_loss: 1.0354 - val\_acc: 0.7399

Epoch 00037: val\_loss did not improve from 1.01281
Epoch 38/10000
100/100 [==============================] - 63s 627ms/step - loss: 0.0012 - acc: 0.9995 - val\_loss: 1.0401 - val\_acc: 0.7397

Epoch 00038: val\_loss did not improve from 1.01281
Epoch 39/10000
100/100 [==============================] - 64s 635ms/step - loss: 2.0456e-04 - acc: 1.0000 - val\_loss: 1.0353 - val\_acc: 0.7373

Epoch 00039: val\_loss did not improve from 1.01281
Epoch 40/10000
100/100 [==============================] - 62s 624ms/step - loss: 0.0039 - acc: 0.9990 - val\_loss: 1.0396 - val\_acc: 0.7382

Epoch 00040: val\_loss did not improve from 1.01281
Epoch 41/10000
100/100 [==============================] - 61s 606ms/step - loss: 1.9365e-04 - acc: 1.0000 - val\_loss: 1.0319 - val\_acc: 0.7403

Epoch 00041: val\_loss did not improve from 1.01281
Epoch 42/10000
100/100 [==============================] - 61s 610ms/step - loss: 2.2626e-04 - acc: 1.0000 - val\_loss: 1.0454 - val\_acc: 0.7354

Epoch 00042: val\_loss did not improve from 1.01281
Epoch 43/10000
100/100 [==============================] - 62s 619ms/step - loss: 1.8857e-04 - acc: 1.0000 - val\_loss: 1.0451 - val\_acc: 0.7356

Epoch 00043: val\_loss did not improve from 1.01281
Epoch 44/10000
100/100 [==============================] - 62s 620ms/step - loss: 2.1045e-04 - acc: 1.0000 - val\_loss: 1.0350 - val\_acc: 0.7378

Epoch 00044: val\_loss did not improve from 1.01281
Epoch 45/10000
100/100 [==============================] - 62s 621ms/step - loss: 8.2094e-04 - acc: 0.9995 - val\_loss: 1.0312 - val\_acc: 0.7436

Epoch 00045: val\_loss did not improve from 1.01281
Epoch 46/10000
100/100 [==============================] - 62s 625ms/step - loss: 2.0650e-04 - acc: 1.0000 - val\_loss: 1.0231 - val\_acc: 0.7442

Epoch 00046: val\_loss did not improve from 1.01281
Epoch 47/10000
100/100 [==============================] - 62s 620ms/step - loss: 9.3536e-04 - acc: 0.9995 - val\_loss: 1.0419 - val\_acc: 0.7436

Epoch 00047: val\_loss did not improve from 1.01281
Epoch 48/10000
100/100 [==============================] - 62s 621ms/step - loss: 0.0019 - acc: 0.9995 - val\_loss: 1.0398 - val\_acc: 0.7416

Epoch 00048: val\_loss did not improve from 1.01281
Epoch 49/10000
100/100 [==============================] - 62s 618ms/step - loss: 0.0025 - acc: 0.9995 - val\_loss: 1.0346 - val\_acc: 0.7419

Epoch 00049: val\_loss did not improve from 1.01281
Epoch 50/10000
100/100 [==============================] - 62s 616ms/step - loss: 0.0017 - acc: 0.9995 - val\_loss: 1.0388 - val\_acc: 0.7406

Epoch 00050: val\_loss did not improve from 1.01281
Epoch 51/10000
100/100 [==============================] - 62s 622ms/step - loss: 0.0034 - acc: 0.9990 - val\_loss: 1.0264 - val\_acc: 0.7487

Epoch 00051: val\_loss did not improve from 1.01281
Epoch 52/10000
100/100 [==============================] - 62s 623ms/step - loss: 1.2724e-04 - acc: 1.0000 - val\_loss: 1.0482 - val\_acc: 0.7416

Epoch 00052: val\_loss did not improve from 1.01281
Epoch 53/10000
100/100 [==============================] - 63s 630ms/step - loss: 0.0015 - acc: 0.9990 - val\_loss: 1.0444 - val\_acc: 0.7412

Epoch 00053: val\_loss did not improve from 1.01281
Epoch 54/10000
100/100 [==============================] - 62s 619ms/step - loss: 1.4778e-04 - acc: 1.0000 - val\_loss: 1.0311 - val\_acc: 0.7414

Epoch 00054: val\_loss did not improve from 1.01281
Epoch 55/10000
100/100 [==============================] - 63s 626ms/step - loss: 6.0311e-04 - acc: 0.9995 - val\_loss: 1.0625 - val\_acc: 0.7365

Epoch 00055: val\_loss did not improve from 1.01281

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogskindresnet.model.weights.best.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{test\PYZus{}datagen} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}\PY{n}{rescale}\PY{o}{=}\PY{l+m+mf}{1.}\PY{o}{/}\PY{l+m+mi}{255}\PY{p}{)}
         \PY{n}{test\PYZus{}generator} \PY{o}{=} \PY{n}{test\PYZus{}datagen}\PY{o}{.}\PY{n}{flow\PYZus{}from\PYZus{}directory}\PY{p}{(}
                 \PY{n}{test\PYZus{}dir}\PY{p}{,}
                 \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{256}\PY{p}{)}\PY{p}{,}
                 \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                 \PY{n}{class\PYZus{}mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{evaluate\PYZus{}generator}\PY{p}{(}\PY{n}{test\PYZus{}generator}\PY{p}{,}\PY{n}{steps}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{max\PYZus{}queue\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{workers}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{use\PYZus{}multiprocessing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Found 836 images belonging to 133 classes.

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} [1.007829131427993, 0.7807692305654542]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{o}{*}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib} \PY{k}{as} \PY{n+nn}{mpl}
         \PY{n}{mpl}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{agg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{acc} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{val\PYZus{}acc} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{loss} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{epochs} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{acc}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{acc}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}acc}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation acc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training and validation accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{loss}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{epochs}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training and validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{yaml\PYZus{}string} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{to\PYZus{}yaml}\PY{p}{(}\PY{p}{)}
         \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./0.yaml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{outfile}\PY{p}{:}
             \PY{n}{outfile}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n}{yaml\PYZus{}string}\PY{p}{)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
